{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../w2v/no_emoji_30_epochs_week5.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "df_test = pd.DataFrame(test)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "df_train = pd.DataFrame(train)\n",
    "\n",
    "val = pd.read_csv('val.csv')\n",
    "df_val = pd.DataFrame(val)\n",
    "\n",
    "all = pd.read_csv('../3 classes/facebook_health_cases (all).csv')\n",
    "df = pd.DataFrame(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaned:  (7907, 2)\n",
      "After:  (7905, 2)\n"
     ]
    }
   ],
   "source": [
    "# remove possible empty text cell\n",
    "df = df[['check_stop', 'sentiment']]\n",
    "\n",
    "print(\"before cleaned: \", df.shape)\n",
    "df['check_stop'].replace('', np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"After: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text = df['check_stop'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化成词向量矩阵，利用新的word2vec模型\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "error_count=0\n",
    "embedding_matrix = np.zeros((vocab_size + 1, 128))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in model.wv:\n",
    "        embedding_matrix[i] = model.wv[word]\n",
    "    else:\n",
    "        error_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.62872031e-01, -4.18042764e-04, -4.52394694e-01, ...,\n",
       "         8.39686692e-02,  2.78790563e-01,  5.80404282e-01],\n",
       "       [-6.51707351e-01,  3.76401782e-01, -4.18396175e-01, ...,\n",
       "         1.41281709e-01,  3.50653052e-01,  9.47910190e-01],\n",
       "       ...,\n",
       "       [-1.18422797e-02,  3.49949636e-02,  1.94209423e-02, ...,\n",
       "         8.95177014e-03,  7.48859392e-03,  2.72510685e-02],\n",
       "       [-5.66299306e-03,  1.42519949e-02,  1.16926571e-02, ...,\n",
       "         1.17878104e-02,  8.84990022e-03,  1.76386032e-02],\n",
       "       [-3.11119203e-03,  1.40225505e-02,  1.50357643e-02, ...,\n",
       "         7.57448794e-03,  3.70327896e-03,  1.97595917e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment'] = pd.Categorical(df_test['sentiment'])\n",
    "df_test['sentiment'] = df_test.sentiment.cat.codes\n",
    "\n",
    "df_train['sentiment'] = pd.Categorical(df_train['sentiment'])\n",
    "df_train['sentiment'] = df_train.sentiment.cat.codes\n",
    "\n",
    "df_val['sentiment'] = pd.Categorical(df_val['sentiment'])\n",
    "df_val['sentiment'] = df_val.sentiment.cat.codes\n",
    "\n",
    "\n",
    "y_test = df_test['sentiment']\n",
    "y_train = df_train['sentiment']\n",
    "y_val = df_val['sentiment']\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_train = df_train['check_stop_no_emoji'].apply(lambda x: x.split())\n",
    "tokenize_test = df_test['check_stop_no_emoji'].apply(lambda x: x.split())\n",
    "tokenize_val = df_val['check_stop_no_emoji'].apply(lambda x: x.split())\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences(tokenize_train)\n",
    "traintitle = pad_sequences(sequence, maxlen=100)\n",
    "sequence = tokenizer.texts_to_sequences(tokenize_val)\n",
    "valtitle = pad_sequences(sequence, maxlen=100)\n",
    "sequence = tokenizer.texts_to_sequences(tokenize_test)\n",
    "testtitle = pad_sequences(sequence, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1,\n",
    "        output_dim=128,\n",
    "        input_length=100,\n",
    "        weights=[embedding_matrix]),\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        #mask_zero=True),\n",
    "    tf.keras.layers.SimpleRNN(64),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 11ms/step - loss: 0.7317 - accuracy: 0.7205 - micro_f1_score: 0.7205 - macro_f1_score: 0.6496\n",
      "Test Loss: 0.7317108511924744\n",
      "Test Accuracy: 0.7205387353897095\n",
      "Micro F1-Score: 0.7205387353897095\n",
      "Macro F1-Score: 0.6495874524116516\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/w2v_rnn_no_emoji.ckpt\"\n",
    "rnn.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1,\n",
    "        output_dim=128,\n",
    "        input_length=100,\n",
    "        weights=[embedding_matrix]),\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        #mask_zero=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 22ms/step - loss: 0.7836 - accuracy: 0.7189 - micro_f1_score: 0.7189 - macro_f1_score: 0.6512\n",
      "Test Loss: 0.7836313247680664\n",
      "Test Accuracy: 0.7188552021980286\n",
      "Micro F1-Score: 0.7188551425933838\n",
      "Macro F1-Score: 0.6511731147766113\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/w2v_simple_lstm_no_emoji.ckpt\"\n",
    "lstm.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULARIZER = tf.keras.regularizers.L2(0.01)\n",
    "\n",
    "bilstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1,\n",
    "        output_dim=128,\n",
    "        input_length=100,\n",
    "        weights=[embedding_matrix],\n",
    "        embeddings_regularizer=REGULARIZER),\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        #mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 32ms/step - loss: 0.8161 - accuracy: 0.7492 - micro_f1_score: 0.7492 - macro_f1_score: 0.6462\n",
      "Test Loss: 0.8160718679428101\n",
      "Test Accuracy: 0.7491582632064819\n",
      "Micro F1-Score: 0.7491582632064819\n",
      "Macro F1-Score: 0.6462236046791077\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/w2v_one_layer_bilstm_no_emoji.ckpt\"\n",
    "bilstm.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer biLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULARIZER = tf.keras.regularizers.L2(0.01)\n",
    "\n",
    "two_bilstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1,\n",
    "        output_dim=128,\n",
    "        input_length=100,\n",
    "        weights=[embedding_matrix],\n",
    "        embeddings_regularizer=REGULARIZER),\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        #mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 59ms/step - loss: 0.8012 - accuracy: 0.7475 - micro_f1_score: 0.7475 - macro_f1_score: 0.6629\n",
      "Test Loss: 0.801200270652771\n",
      "Test Accuracy: 0.747474730014801\n",
      "Micro F1-Score: 0.7474746704101562\n",
      "Macro F1-Score: 0.6628547310829163\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/w2v_two_layer_bilstm_no_emoji.ckpt\"\n",
    "two_bilstm.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer biLSTM + dropout 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULARIZER = tf.keras.regularizers.L2(0.01)\n",
    "\n",
    "two_bilstm_02 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1,\n",
    "        output_dim=128,\n",
    "        input_length=100,\n",
    "        weights=[embedding_matrix],\n",
    "        embeddings_regularizer=REGULARIZER),\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        #mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 57ms/step - loss: 0.7869 - accuracy: 0.7525 - micro_f1_score: 0.7525 - macro_f1_score: 0.6654\n",
      "Test Loss: 0.7869081497192383\n",
      "Test Accuracy: 0.752525269985199\n",
      "Micro F1-Score: 0.7525252103805542\n",
      "Macro F1-Score: 0.665414571762085\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/w2v_two_layer_bilstm_02_no_emoji.ckpt\"\n",
    "two_bilstm_02.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULARIZER = tf.keras.regularizers.L2(0.01)\n",
    "\n",
    "two_bilstm_02_batch = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1,\n",
    "        output_dim=128,\n",
    "        input_length=100,\n",
    "        weights=[embedding_matrix],\n",
    "        embeddings_regularizer=REGULARIZER),\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        #mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 32ms/step - loss: 0.8832 - accuracy: 0.7357 - micro_f1_score: 0.7357 - macro_f1_score: 0.6705\n",
      "Test Loss: 0.8832367658615112\n",
      "Test Accuracy: 0.7356902360916138\n",
      "Micro F1-Score: 0.735690176486969\n",
      "Macro F1-Score: 0.670479953289032\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/w2v_two_layer_bilstm_02_batch_no_emoji.ckpt\"\n",
    "two_bilstm_02_batch.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
