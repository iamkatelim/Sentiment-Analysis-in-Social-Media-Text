{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/text/text_classification_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import losses\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "df_test = pd.DataFrame(test)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "df_train = pd.DataFrame(train)\n",
    "\n",
    "val = pd.read_csv('val.csv')\n",
    "df_val = pd.DataFrame(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment'] = pd.Categorical(df_test['sentiment'])\n",
    "df_test['sentiment'] = df_test.sentiment.cat.codes\n",
    "\n",
    "df_train['sentiment'] = pd.Categorical(df_train['sentiment'])\n",
    "df_train['sentiment'] = df_train.sentiment.cat.codes\n",
    "\n",
    "df_val['sentiment'] = pd.Categorical(df_val['sentiment'])\n",
    "df_val['sentiment'] = df_val.sentiment.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.keras.utils.to_categorical(df_train['sentiment'], num_classes=3)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df_train.check_stop_no_emoji.values, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.keras.utils.to_categorical(df_val['sentiment'], num_classes=3)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((df_val.check_stop_no_emoji.values, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.keras.utils.to_categorical(df_test['sentiment'], num_classes=3)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((df_test.check_stop_no_emoji.values, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = int(df_train.shape[0])\n",
    "val_ds_size = int(df_val.shape[0])\n",
    "test_ds_size = int(df_test.shape[0])\n",
    "\n",
    "train_data = dataset.take(train_ds_size)\n",
    "val_data = dataset_val.take(val_ds_size)\n",
    "test_data = dataset_test.take(test_ds_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7114\n",
      "197\n",
      "594\n"
     ]
    }
   ],
   "source": [
    "print(len(list(train_data.as_numpy_iterator())))\n",
    "print(len(list(val_data.as_numpy_iterator())))\n",
    "print(len(list(test_data.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'please allow dine restaurants food premises people take granted lepak'\n",
      " b'salahkan kerajaan tak pkp baik pndg diri bila buka sempadan negeri berjalan tidak covid kerajaan tak kena pkp salah tidak pkp salah larangan rentas negeri salah tak larangan rentas negeri salah'\n",
      " b'alhamduli semoga esok banyak berkurangan amin syabas']\n",
      "\n",
      "labels:  [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for example, label in val_dataset.take(1):\n",
    "    print('texts: ', example.numpy()[:3])\n",
    "    print()\n",
    "    print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'terima kasih rakyat malaysia membantu menjayakan penurunan angka kes baharu kekalkan patuhi sop'\n",
      " b'dear government beautician hairstylists industry already eat grass no money no income'\n",
      " b'data health ministry indicates number people getting tested covid dropped people tested feb people tested sunday daily testing dropped not sure comparing correctly']\n",
      "\n",
      "labels:  [[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for example, label in test_dataset.take(1):\n",
    "    print('texts: ', example.numpy()[:3])\n",
    "    print()\n",
    "    print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=10155\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10155"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .adapt method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'kes', 'tidak', 'pkp', 'covid', 'boleh', 'tak',\n",
       "       'turun', 'alhamdulillah', 'rakyat', 'tapi', 'sop', 'negeri',\n",
       "       'baru', 'kena', 'semoga', 'jangan', 'bila', 'banyak'], dtype='<U32')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed output_sequence_length):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  75,   84,   10,   22,  411, 7926,  134,   32,    2,  135,  684,\n",
       "         251,   12,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [1858,  243,    1,    1, 3260,  723, 4693,    1,   91, 2449,   91,\n",
       "         924,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [ 307,  992, 2046, 9185,  343,  188,  656,  951,    5, 2617,  188,\n",
       "         951,  625,  188,  951, 5979,  494,  464, 2617,   95, 1117,    1,\n",
       "        4820,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
    "\n",
    "1. The default value for preprocessing.TextVectorization's standardize argument is \"lower_and_strip_punctuation\".\n",
    "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  b'terima kasih rakyat malaysia membantu menjayakan penurunan angka kes baharu kekalkan patuhi sop'\n",
      "Round-trip:  terima kasih rakyat malaysia membantu menjayakan penurunan angka kes baharu kekalkan patuhi sop                                        \n",
      "\n",
      "Original:  b'dear government beautician hairstylists industry already eat grass no money no income'\n",
      "Round-trip:  dear government [UNK] [UNK] industry already eat [UNK] no money no income                                         \n",
      "\n",
      "Original:  b'data health ministry indicates number people getting tested covid dropped people tested feb people tested sunday daily testing dropped not sure comparing correctly'\n",
      "Round-trip:  data health ministry indicates number people getting tested covid dropped people tested feb people tested sunday daily testing dropped not sure [UNK] correctly                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    print(\"Original: \", example[n].numpy())\n",
    "    print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bidirectional.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.7244 - accuracy: 0.7239 - micro_f1_score: 0.7239 - macro_f1_score: 0.6457\n",
      "lss: 0.7244\n",
      "acc: 0.7239\n",
      "micro f1: 0.7239\n",
      "macro f1: 0.6457\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/one_bilstm_no_emoji.ckpt\"\n",
    "bilstm.load_weights(checkpoint_path)\n",
    "loss,acc,micro_f1,macro_f1 = bilstm.evaluate(test_dataset, verbose = 2, batch_size = 32)\n",
    "print(\"lss: %.4f\" % (loss))\n",
    "print(\"acc: %.4f\" % (acc))\n",
    "print(\"micro f1: %.4f\" % (micro_f1))\n",
    "print(\"macro f1: %.4f\" % (macro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.SimpleRNN(64),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.8282 - accuracy: 0.6852 - micro_f1_score: 0.6852 - macro_f1_score: 0.6264\n",
      "lss: 0.8282\n",
      "acc: 0.6852\n",
      "micro f1: 0.6852\n",
      "macro f1: 0.6264\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/simple_rnn_no_emoji.ckpt\"\n",
    "rnn.load_weights(checkpoint_path)\n",
    "loss,acc,micro_f1,macro_f1 = rnn.evaluate(test_dataset, verbose = 2, batch_size = 32)\n",
    "print(\"lss: %.4f\" % (loss))\n",
    "print(\"acc: %.4f\" % (acc))\n",
    "print(\"micro f1: %.4f\" % (micro_f1))\n",
    "print(\"macro f1: %.4f\" % (macro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        #embeddings_regularizer = tf.keras.regularizers.L2(0.01),\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.8427 - accuracy: 0.7306 - micro_f1_score: 0.7306 - macro_f1_score: 0.6642\n",
      "lss: 0.8427\n",
      "acc: 0.7306\n",
      "micro f1: 0.7306\n",
      "macro f1: 0.6642\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/simple_lstm_no_emoji.ckpt\"\n",
    "lstm.load_weights(checkpoint_path)\n",
    "loss,acc,micro_f1,macro_f1 = lstm.evaluate(test_dataset, verbose = 2, batch_size = 32)\n",
    "print(\"lss: %.4f\" % (loss))\n",
    "print(\"acc: %.4f\" % (acc))\n",
    "print(\"micro f1: %.4f\" % (micro_f1))\n",
    "print(\"macro f1: %.4f\" % (macro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_bilstm = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.7902 - accuracy: 0.7071 - micro_f1_score: 0.7071 - macro_f1_score: 0.6215\n",
      "lss: 0.7902\n",
      "acc: 0.7071\n",
      "micro f1: 0.7071\n",
      "macro f1: 0.6215\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/two_bilstm_emoji.ckpt\"\n",
    "two_bilstm.load_weights(checkpoint_path)\n",
    "loss,acc,micro_f1,macro_f1 = two_bilstm.evaluate(test_dataset, verbose = 2, batch_size = 32)\n",
    "print(\"lss: %.4f\" % (loss))\n",
    "print(\"acc: %.4f\" % (acc))\n",
    "print(\"micro f1: %.4f\" % (micro_f1))\n",
    "print(\"macro f1: %.4f\" % (macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn: \n",
      "[ 1.6851637 -1.3144987 -1.4902252]\n",
      "lstm: \n",
      "[ 1.8899117 -1.5894849 -1.7842244]\n",
      "one layer bilstm: \n",
      "[ 1.3816768 -0.7655858 -1.315508 ]\n",
      "two layer bilstm: \n",
      "[-0.574112   0.8526867 -1.0519863]\n"
     ]
    }
   ],
   "source": [
    "sample_text = ('relax malaysia still million peoples not infected')\n",
    "\n",
    "predictions = rnn.predict(np.array([sample_text]))\n",
    "print(\"rnn: \")\n",
    "print(predictions[0])\n",
    "\n",
    "predictions = lstm.predict(np.array([sample_text]))\n",
    "print(\"lstm: \")\n",
    "print(predictions[0])\n",
    "\n",
    "predictions = bilstm.predict(np.array([sample_text]))\n",
    "print(\"one layer bilstm: \")\n",
    "print(predictions[0])\n",
    "\n",
    "predictions = two_bilstm.predict(np.array([sample_text]))\n",
    "print(\"two layer bilstm: \")\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
